Metadata-Version: 2.4
Name: agent-server
Version: 0.1.0
Summary: LangGraph + LiteLLM Agent Server (MVP)
Author-email: tc_reg <dev@example.com>
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.115.0
Requires-Dist: uvicorn[standard]>=0.30.0
Requires-Dist: litellm>=1.42.10
Requires-Dist: langgraph>=0.2.39
Requires-Dist: langchain-core>=0.3.7
Requires-Dist: pydantic>=2.7.0
Requires-Dist: pydantic-settings>=2.5.2
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: httpx>=0.27.0
Requires-Dist: chromadb>=0.5.5
Requires-Dist: pandas>=2.2.2
Requires-Dist: aiofiles>=23.2.1
Requires-Dist: streamlit>=1.37.0
Requires-Dist: python-multipart>=0.0.20
Requires-Dist: cryptography>=42.0.0
Provides-Extra: dev
Requires-Dist: pytest>=8.3.2; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23.8; extra == "dev"
Requires-Dist: httpx[http2]>=0.27.0; extra == "dev"
Requires-Dist: anyio>=4.4.0; extra == "dev"

# Agent Server (MVP)

LangGraph + LiteLLM agent server with FastAPI. Provides:
- Chat app (optional LocalRAG retrieval via session_id)
- CSV/Folder analysis app (ingest → index → retrieve → generate → write)

## Prerequisites
- Python 3.11
- uv (recommended) or pip

## Setup (uv)
```bash
uv venv
source .venv/bin/activate  # Windows: .venv\\Scripts\\activate
uv pip install -e .[dev]
cp env.example .env  # fill secrets
```

## Run
```bash
uv run uvicorn src.server.main:app --reload
```

## UI (Streamlit)
```bash
# optional: set API_BASE_URL in .streamlit/secrets.toml or env
cp .streamlit/secrets.toml.example .streamlit/secrets.toml
uv run streamlit run ui/app.py --server.address 0.0.0.0 --server.port 8501
```

## Remote Access
- Start API on 0.0.0.0: `uv run uvicorn src.server.main:app --host 0.0.0.0 --port 8000`
- Start UI on 0.0.0.0: `uv run streamlit run ui/app.py --server.address 0.0.0.0 --server.port 8501`
- From a remote machine:
  - Open `http://<SERVER_IP>:8501` (UI)
  - Set UI `API_BASE_URL` to `http://<SERVER_IP>:8000` in `.streamlit/secrets.toml`
- File downloads:
  - The API exposes `GET /api/v1/files/{session_id}/{filepath}` for any file under `OUTPUT_DIR/{session_id}`.
  - CSV process responses include `file_urls` (relative). The UI links to them using your `API_BASE_URL`.

### CORS
- Configure allowed origins via env: `CORS_ORIGINS=*` (default) or `http://localhost:8501,http://<SERVER_IP>:8501`

### Optional public tunneling
- Cloudflare: `cloudflared tunnel --url http://localhost:8501`
- Ngrok: `ngrok http 8501`

## Environment (.env)
See `env.example`.

Key variables:
- OPENAI_API_KEY, OPENAI_BASE
- LLM_MODEL_ID (default: gpt-4o-mini)
- LOG_DIR, OUTPUT_DIR, DATA_DIR, CHROMA_DB_DIR
- HYBRID_SEARCH_ENABLED (default: true)
- SQLITE_DB_PATH (default: ./data/indices/sqlite/app.db)
- SQL_AGENT_ENABLED (default: true)
- SQL_MAX_ROWS (default: 200)
- DB_CONTEXT_ENABLED (default: true)
- DB_CONTEXT_MAX_TOKENS (default: 512)

### H Chat (Claude) via personal API key
- Enable by env: `HCHAT_ENABLED=true`
- Configure base URL: `HCHAT_BASE_URL=https://h-chat-api.autoever.com/v2/api`
- Provide key: `HCHAT_API_KEY=<your_key>`
- Provider (future-proof): `HCHAT_PROVIDER=claude`
- Auth style (optional): `HCHAT_AUTH_STYLE=bearer | api-key | raw-authorization` (default sends both bearer+api-key)
- Set model id (e.g.): `LLM_MODEL_ID=claude-3-5-sonnet-v2`
- When enabled, all chat completions route to H Chat Claude endpoint (`{HCHAT_BASE_URL}/claude/messages`). Streaming is available internally for future use.

Example models:
- `LLM_MODEL_ID=claude-haiku-4-5` (matches curl example)
- Or pass per-request `model_id` in API body for `/api/v1/apps/chat/process`

Refs:
- H Chat Overview: https://h-chat-docs.autoever.com/en/guide/get-started/overview/
- H Chat Personal API: https://h-chat-docs.autoever.com/en/guide/pro-only/personal-key/overview/

## API
- GET `/api/v1/health`
- POST `/api/v1/apps/chat/process`
  - body: `{ query, system_prompt?, model_id?, session_id?, k?, retrieval_mode? }`
- POST `/api/v1/apps/chat/ingest`
  - form-data: `files=[UploadFile]*` or `folder_zip`
  - returns `{ session_id, doc_count }`
- POST `/api/v1/apps/csv/ingest`
  - form-data: `files=[UploadFile]*` or `folder_zip`
  - returns `{ session_id, doc_count }`
- POST `/api/v1/apps/csv/process`
  - body: `{ session_id, query, k?, model_id? }`
  - returns `{ answer, files, sources?, model_id }`

## Hybrid Search (Chroma + SQLite FTS5)
- Dense retrieval: Chroma persistent store under `CHROMA_DB_DIR`.
- Keyword/structured: SQLite FTS5 at `SQLITE_DB_PATH`.
- Toggle via `HYBRID_SEARCH_ENABLED` (graphs pick HybridRAG automatically).
- Ingest:
  - CSV/TXT/MD are chunked and stored into SQLite (rows + FTS).
  - CSV files are analyzed for schema and stored under `schema_columns`.
  - Chunks are indexed into Chroma as before.
- Query:
  - Both stores are searched and results fused (RRF), then passed as context to the LLM.

## Intent-Gated SQL + Hybrid
- Intent agent classifies queries: none | sql | hybrid | both (override via `retrieval_mode`).
- SQL agent:
  - Generates safe SELECT-only SQLite for tables: `schema_columns`, `files`, `rows`, `fts_rows` (scoped by `session_id`).
  - Enforces read-only, injects LIMIT (default `SQL_MAX_ROWS`), times out on long queries.
  - Adds a compact SQL summary to context; responses include a `sql` source entry.
- DB context:
  - A compact per-session summary of files, columns and types, and row counts.
  - Used to bias intent classification and included as a `[DB]` section in prompts.

## Tests
```bash
uv run pytest -q
```


